{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can more efficiently explore this space by:\n",
    "1. Obtaining the Out Of Fold (OOF) predictions for the models we want to combine\n",
    "2. Selecting combinations of single models\n",
    "3. Scoring the simple average of the results\n",
    "\n",
    "Doing it this way allows us to try more combinations and use a local CV score, instead of the public leaderboard, to inform decisions on which model combinations work well together.\n",
    "\n",
    "The random seed is used to determine the CV folds that generate the OOF predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from math import factorial\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from exp.train import train_model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from exp.features import load_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_results= \"exp1.csv\"\n",
    "score_df = load_cv_results(save_results)\n",
    "score_df['filename'] = save_results\n",
    "\n",
    "save_results= \"exp2.csv\"\n",
    "tmp = load_cv_results(save_results)\n",
    "tmp['filename'] = save_results\n",
    "score_df = score_df.append(tmp)\n",
    "\n",
    "save_results= \"exp3.csv\"\n",
    "tmp = load_cv_results(save_results)\n",
    "tmp['filename'] = save_results\n",
    "score_df = score_df.append(tmp)\n",
    "score_df[\"feature_set\"] = \"standard_scaled\"\n",
    "\n",
    "save_results= \"exp4_1.csv\"\n",
    "score_df_ = load_cv_results(save_results)\n",
    "score_df_['filename'] = save_results\n",
    "score_df_ = score_df_.rename(index=str, columns={\"cv_score_n_folds_10_shuffle_True_rs_41\": \"score\"})\n",
    "score_df_['n_folds'] = 10\n",
    "score_df_['shuffle'] = True\n",
    "score_df_['rs'] = 41\n",
    "score_df = score_df.append(score_df_)\n",
    "\n",
    "save_results= \"exp5.csv\"\n",
    "score_df_ = load_cv_results(save_results)\n",
    "score_df_['filename'] = save_results\n",
    "score_df_ = score_df_.rename(index=str, columns={\"cv_score_n_folds_10_shuffle_True_rs_41\": \"score\"})\n",
    "score_df_['n_folds'] = 10\n",
    "score_df_['shuffle'] = True\n",
    "score_df_['rs'] = 41\n",
    "score_df = score_df.append(score_df_)\n",
    "\n",
    "save_results= \"exp6.csv\"\n",
    "score_df_ = load_cv_results(save_results)\n",
    "score_df_['filename'] = save_results\n",
    "score_df_ = score_df_.rename(index=str, columns={\"cv_score_n_folds_10_shuffle_True_rs_41\": \"score\"})\n",
    "score_df_['n_folds'] = 10\n",
    "score_df_['shuffle'] = True\n",
    "score_df_['rs'] = 41\n",
    "score_df = score_df.append(score_df_)\n",
    "\n",
    "save_results= \"exp7.csv\"\n",
    "score_df_ = load_cv_results(save_results)\n",
    "score_df_['filename'] = save_results\n",
    "score_df_ = score_df_.rename(index=str, columns={\"cv_score_n_folds_10_shuffle_True_rs_41\": \"score\"})\n",
    "score_df_['n_folds'] = 10\n",
    "score_df_['shuffle'] = True\n",
    "score_df_['rs'] = 41\n",
    "score_df = score_df.append(score_df_)\n",
    "\n",
    "save_results= \"exp8_1.csv\"\n",
    "score_df_ = load_cv_results(save_results)\n",
    "score_df_['filename'] = save_results\n",
    "score_df_ = score_df_.rename(index=str, columns={\"cv_score_n_folds_10_shuffle_True_rs_41\": \"score\"})\n",
    "score_df_['n_folds'] = 10\n",
    "score_df_['shuffle'] = True\n",
    "score_df_['rs'] = 41\n",
    "score_df = score_df.append(score_df_)\n",
    "\n",
    "save_results= \"exp8_2.csv\"\n",
    "score_df_ = load_cv_results(save_results)\n",
    "score_df_['filename'] = save_results\n",
    "score_df_ = score_df_.rename(index=str, columns={\"cv_score_n_folds_6_shuffle_False_rs_41\": \"score\"})\n",
    "score_df_['n_folds'] = 6\n",
    "score_df_['shuffle'] = False\n",
    "score_df_['rs'] = 41\n",
    "score_df = score_df.append(score_df_)\n",
    "\n",
    "save_results= \"exp9.csv\"\n",
    "score_df_ = load_cv_results(save_results)\n",
    "score_df_['filename'] = save_results\n",
    "score_df_ = score_df_.rename(index=str, columns={\"cv_score_n_folds_5_shuffle_False_rs_None\": \"score\"})\n",
    "score_df_['n_folds'] = 5\n",
    "score_df_['shuffle'] = False\n",
    "score_df_['rs'] = None\n",
    "score_df = score_df.append(score_df_)\n",
    "\n",
    "score_df.reset_index(inplace=True)\n",
    "\n",
    "score_df['model_id'] = score_df['index'].astype(str)\n",
    "score_df['model_id'] = score_df[['alg','filename','model_id']].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "score_df = score_df.sort_values(by=\"score\", axis=0)\n",
    "\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain OOF Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the Out Of Fold (OOF) predictions for the models we want to combine.  We can try only our top performing single models, or deliberately include poorer performing single models that introduce model heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_24k = score_df['feature_set'].apply(lambda x: not x.startswith('2400'))\n",
    "non_24k = score_df.loc[non_24k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_24k.sort_values('score', inplace=True)\n",
    "non_24k.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the top 15 best scoring single models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OOF_list = non_24k['score'].head(15).index\n",
    "OOF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_params(model_exp):\n",
    "    model_dict = {}\n",
    "    \n",
    "    model_dict['params'] = json.loads(model_exp['params_json'])\n",
    "    model_dict['fs'] = model_exp['feature_set']\n",
    "    model_dict['alg'] = model_exp['alg']\n",
    "    model_dict['rs'] = RANDOM_SEED\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_params = []\n",
    "for model_idx in OOF_list:\n",
    "    oof_params.append(get_train_params(score_df.loc[model_idx]))\n",
    "\n",
    "oof_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get OOF Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oof_preds = {}\n",
    "i=0\n",
    "for model_dict in oof_params:\n",
    "    _,_, oof = train_model(**model_dict)\n",
    "    oof_preds[OOF_list[i]] = oof\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds_df = pd.DataFrame(oof_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds_df.to_csv('oof_preds_rs42.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating all combinations does not scale.\n",
    "\n",
    "$N$: The pool of models to select combinations from\n",
    "\n",
    "$k$: The number of single models in an ensemble \n",
    "\n",
    "$$\\frac{N!}{k!(N-k)!}$$\n",
    "\n",
    "Will have to choose a smaller N, or smaller k, in order to explore the space, or use an algorithm that is only gauraunteed to find a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_average(ensemble_oof_preds):\n",
    "    return mean_absolute_error(ttf, np.mean(ensemble_oof_preds.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttf = pd.read_csv('../kaggle_files/features/train/ttf.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# N = 20 \n",
    "k_range = range(2,15)\n",
    "\n",
    "all_combinations = []\n",
    "for k in k_range:\n",
    "    print(k)\n",
    "#     top_k = itertools.combinations(oof_preds_df.columns[:N],k)\n",
    "    top_k = itertools.combinations(oof_preds_df.columns,k)\n",
    "    for combo in top_k:\n",
    "        all_combinations.append([combo,simple_average(oof_preds_df[list(combo)])])\n",
    "\n",
    "all_combinations_df = pd.DataFrame(all_combinations, columns=['models','mean_cv'])\n",
    "\n",
    "all_combinations_df.sort_values(by='mean_cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = score_df.loc[[1810, 825, 720, 1879, 1792, 811]]\n",
    "ensemble.to_csv('ensemble2_models.csv')\n",
    "\n",
    "ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Test Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.train import load_train_features, load_test_features, train_get_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "y_preds = {}\n",
    "for idx,single_model in ensemble.iterrows():\n",
    "    print(single_model['alg'])\n",
    "    X_tr, y_tr = load_train_features(set = single_model['feature_set'])\n",
    "    X_test = load_test_features(set = single_model['feature_set'])\n",
    "    params = json.loads(single_model['params_json'])\n",
    "    alg = single_model['alg']\n",
    "    model_id = single_model['model_id']\n",
    "\n",
    "    model, y_pred = train_get_test_preds(X_tr, y_tr, X_test, params=params, alg=alg)\n",
    "    \n",
    "    y_preds[model_id] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../kaggle_files/submission/sample_submission.csv')\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = pd.DataFrame(y_preds).T.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('ensemble2_preds.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
